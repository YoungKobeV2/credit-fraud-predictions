---
title: "Credit-Fraud-Detection"
author: "Lebintiti Kobe"
date: "`r Sys.Date()`"
output:
  bookdown::gitbook:
    
    split_by: section
    number_sections: F
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = F,error = F,message = F)
knitr::opts_chunk$set(tidy = T, tidy.opts = list(width.cutoff=55,blanks=T))
knitr::opts_chunk$set(fig.width = 10,fig.height = 7,dpi = 700)

require(patchwork)
require(ggplot2)
require(ggthemes)
require(extrafont)

theme_set(
  theme_excel_new(base_size = 13, base_family = "Tahoma") +
    theme(
      plot.title = element_text(size = 17, hjust = 0, margin = margin(b = 5), face = 2),
      plot.subtitle = element_text(size = 13, hjust = 0, margin = margin(b = 20)),
      axis.title.x = element_text(margin = , face = 4),
      axis.title.y = element_text(margin = margin(r = 10), face = 4),
      legend.key.height = unit(0.2, "lines"),
      legend.key.width = unit(10, "lines")
    )
  
)
```

## Goal of this section 

-  Use features selected from EDA.Rmd to build our predictive models  

## About the data 

This dataset provides fully anonymized features derived from multiple sources to enable participants to develop predictive models while ensuring data privacy. The goal is to predict target (first payment default) using various financial, employment, behavioral, and location-based features.

## load the data 

```{r}
require(here)
require(tidyverse)
require(tidymodels)
require(themis)
require(finetune)


data <- read.csv(here("data/kaggle_dataset.csv")) %>% as_tibble()


data <- data %>%
  relocate(target) %>%
  mutate(target = as.factor(target))

data 
```

## split the data 

Split data into test and train data , and data we going to use to test our api

```{r}
set.seed(545672)

api_splits <- rsample::initial_split(data,strata = target)

test_api <- testing(api_splits)
test_api

data2 <- training(api_splits)

splits <- rsample::initial_split(data2,strata = target,prop = 0.7)
train <- training(splits)
test <- testing(splits)
```

Create fold for model tuning

```{r}
train_folds <- vfold_cv(train,strata = target,v = 2,repeats = 5)
train_folds
```

## Recipe

-  Load selected variables 

```{r}
all_selected_features <- read.csv("selected_features_eda.csv")$x
all_selected_features

train_features <- read.csv("train_features_eda.csv")$x
train_features

variables_to_remove<- read.csv("features_to_remove_eda.csv")$x
variables_to_remove

interactions_terms <- read.csv("interaction_terms_eda.csv")$x
interactions_terms

interactions_formula <- readRDS("interaction_formula_eda.rds")
interactions_formula
```

## Recipes

```{r}

# Recipe with no downsampling or smote 
recipe_norm <- train %>%
  select(all_of(train_features)) %>%
  recipe(target ~ .) %>%
  step_indicate_na(all_predictors()) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_interact(terms = interactions_formula, sep = ":") %>%
  step_rm(all_of(variables_to_remove)) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_predictors())

# verify if the selected features and interactions are indeed in our JUICED data 
df_norm <- recipe_norm %>% prep %>% juice()
all_selected_features[-grep(":",all_selected_features)] %in% colnames(df_norm)
all_selected_features[grep(":", all_selected_features)] %>%
  as_tibble() %>%
  bind_cols(interactions_terms %>% as_tibble())

# Check how the class imbalance is in this recipe_norm JUICED data 
train$target %>% table()
df_norm$target %>% table()


# recipe with downsampling 
recipe_down <- train %>%
  select(all_of(train_features)) %>%
  recipe(target ~ .) %>%
  step_indicate_na(all_predictors()) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_interact(terms = interactions_formula, sep = ":") %>%
  step_rm(all_of(variables_to_remove)) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_downsample(target, seed = 123)

# verify if the selected features and interactions are indeed in our JUICED data
df_down <- recipe_down %>% prep %>% juice()
all_selected_features[-grep(":", all_selected_features)] %in% colnames(df_down)

all_selected_features[grep(":", all_selected_features)] %>%
  as_tibble() %>%
  bind_cols(interactions_terms %>% as_tibble())

# Check how the class imbalance is in this recipe_down JUICED data (should be no imbalacnce)
train$target %>% table()
df_down$target %>% table()
```

Create a recipe that includes smote to try and minimize severity of the class imbalance 

-  To create this we first have to tune smote recipe steps to find the best number of neighbours for the smote algorithms 

```{r}
log_spec <- logistic_reg()

folds <- vfold_cv(train, strata = target, v = 2, repeats = 3)
folds

results_smote <- list()

neighbors_values <- seq(3, 21, 2) #neighbours values should be prime numbers 
neighbors_values

# Use a for loop to tune for the best value of neighbours 
for (n in neighbors_values) {
  recipe_smote_tune <- train %>%
    select(all_of(train_features)) %>%
    recipe(target ~ .) %>%
    step_indicate_na(all_predictors()) %>%
    step_impute_median(all_numeric_predictors()) %>%
    step_interact(terms = interactions_formula, sep = ":") %>%
    step_rm(all_of(variables_to_remove)) %>%
    step_YeoJohnson(all_numeric_predictors()) %>%
    step_normalize(all_predictors()) %>%
    step_smote(all_outcomes(), over_ratio = 0.2, neighbors = n) %>%
    step_bsmote(all_outcomes(), over_ratio = 0.1, neighbors = n)

  log_wf <- workflow(recipe_smote_tune, log_spec)

  fit <- fit_resamples(log_wf, resamples = folds)

  results_smote[[paste(n)]] <- fit %>%
    collect_metrics() %>%
    filter(.metric == "roc_auc") %>%
    pull(mean)
}

# We are going to use the value that gives the best roc_auc
results_smote %>% unlist()

best_K <- results_smote %>%
  unlist() %>%
  as_tibble(rownames = "K") %>%
  slice_max(order_by = value) %>%
  pull(K) %>%
  as.numeric()

# recipe smote 
recipe_smote <- train %>%
  select(all_of(train_features)) %>%
  recipe(target ~ .) %>%
  step_indicate_na(all_predictors()) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_interact(terms = interactions_formula, sep = ":") %>%
  step_rm(all_of(variables_to_remove)) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_smote(all_outcomes(), over_ratio = 0.2, neighbors = best_K) %>%
  step_bsmote(all_outcomes(), over_ratio = 0.1, neighbors = best_K)

df_smote <- recipe_smote %>% prep %>% juice()

# check how the class imbalance is after applying smote 
train$target %>% table()
df_smote$target %>% table()
```

## MODEL SPECIFICATIONS 

-  Create diiferent models which we will tune and train then choose the one with best performance .

```{r}
log_spec <- logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

knn_spec <- nearest_neighbor(neighbors = tune()) %>%
  set_mode("classification") %>%
  set_engine("kknn")

rand_spec <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_mode("classification") %>%
  set_engine("randomForest")

xgb_spec <- boost_tree(
  mtry = tune(),
  trees = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune()
) %>%
  set_mode("classification") %>%
  set_engine("xgboost")
```

## Workflows  

- running the commented out lines will take us around 5 hours , 
- best model is xgb_model and best recipe is recipe_norm to save yourself some time . 

```{r}
wf_set <- workflow_set(
  preproc = list(
    recipe_smote = recipe_smote,
    #                                      recipe_norm=recipe_norm,
    #                                       recipe_down=recipe_down
  ),
  #
  models = list(
    #                       log_model = log_spec,
    xgb_model = xgb_spec
  ),
  cross = T
)

#
results <- workflow_map(wf_set,
  resamples = train_folds,
  grid = 10, seed = 123,
  control = control_grid(save_workflow = T),
  verbose = T
)
results

results %>%
  autoplot(metric = "roc_auc")

results %>%
  rank_results(select_best = T) %>%
  filter(.metric == "roc_auc") %>%
  select(wflow_id, .metric, mean, std_err)
```

```{r}
best_xgb_tune <- results %>%
  extract_workflow_set_result(id = "recipe_norm_xgb_model") %>%
  select_best()
best_xgb_tune

xgb_wf <- finalize_workflow(
  workflow(recipe_norm, xgb_spec),
  best_xgb_tune
)
xgb_wf

best_model <- last_fit(xgb_wf, split = splits)
best_model

best_model %>% collect_metrics()

roc_data <- best_model %>%
  unnest(.predictions) %>%
  roc_curve(truth = target, .pred_0) %>%
  mutate(
    youden_index = (sensitivity + specificity) / 2,
    top_left = sqrt((1 - 1.1 * sensitivity)^2 + (1 - specificity)^2)
# i messed with top_left metric so we get threshold that will lead to balanced accuracy . 
  )
# ROC plot
roc_data %>% autoplot()

roc_data %>%
  slice_min(order_by = top_left)
roc_data %>%
  slice_max(order_by = youden_index)

# I choose threshold that leads to balanced accuracy 
thres <- roc_data %>%
  slice_min(order_by = top_left) %>%
  pull(.threshold)

thres <- 1 - thres
thres

wf <- extract_workflow(best_model)

model <- fit(object = wf, data = data2)

classes1 <- predict(object = model, new_data = test_api) %>%
  mutate(predict(object = model, new_data = test_api, type = "prob"),
    youden_class = ifelse(.pred_1 > thres, 1, 0),
    actual = test_api$target
  )
classes1

caret::confusionMatrix(
  data = classes1$youden_class %>% factor(),
  reference = test_api$target %>% factor()
)
```
-  The model performs at a balanced accuracy of about 60% . Im happy with that .


## Save the model and other important files to help with API Building 
```{r}
setwd(here("api/"))

saveRDS(model, "model.rds")
class_threshold <- thres %>% as_tibble()
write.csv(class_threshold, "classification_threshold.csv", row.names = F)
```

